# type 종류, hf, ollama, openai
# 기타 파라미터들
# - 필수: model
# - 선택: temperature, base_url, api_key
# 허깅페이스의 경우 api_key와 HUGGINGFACEHUB_API_TOKEN 환경변수 둘다 없을때, 로컬 허깅페이스를 탐색.
# 허깅페이스 temperature: 1e-5 가 가장 작은값

# vLLM의 경우 아래와 같이 하기
#  type: openai
#  model: ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g
#  temperature: 0.0
#  base_url: http://172.17.0.1:8002/v1
#  api_key: _EMPTY_

llm:
  tiers:
    small:
#      type: ollama
#      model: qwen3-30b-a3b-q5-k-m # skt-ax40-q4-k-m # sktax40l_q5_k_m
#      temperature: 0.0 # 기본값이니 손대지 않기. policy에서 bind하며 조정함
#      base_url: http://127.0.0.1:11434
      type: openai
      model: ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g
      temperature: 0.0
      base_url: http://127.0.0.1:8002/v1
      api_key: _EMPTY_
    large:
#      type: ollama
#      model: qwen3-30b-a3b-q5-k-m # skt-ax40-q4-k-m # sktax40l_q5_k_m
#      temperature: 0.0 # 기본값이니 손대지 않기. policy에서 bind하며 조정함
#      base_url: http://127.0.0.1:11434
      type: openai
      model: ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g
      temperature: 0.0
      base_url: http://127.0.0.1:8002/v1
      api_key: _EMPTY_

policy:
  # 역할별 기본 티어
  default_tier_by_role:
    clarify: small
    planner: small
    query_expand: small
    fact_summary: small
    sufficiency: small
    synthesize: large

  # 복잡한 포맷/지시 단계는 항상 large
  force_large_roles: ["synthesize"]

  # 생성 파라미터 프리셋
  generation_presets:
    deterministic: {temperature: 0.0, top_p: .1}
    focused:       {temperature: 0.2, top_p: .8}
    balanced:      {temperature: 0.3, top_p: .9}
    exploratory:   {temperature: 0.7, top_p: .9}

  # 역할별 프리셋 매핑
  role_generation:
    clarify: deterministic
    planner: deterministic
    query_expand: focused
    fact_summary: focused
    sufficiency: deterministic
    synthesize: balanced

  # 오토스케일/가드
  autoscale:
    enabled: true
    retry_limit_per_step: 1
    max_large_ratio: 0.15
    triggers:
      json_fail: 1
      short_summary_min_chars: 220
      token_threshold: 2000
      repeated_insufficient: 2
