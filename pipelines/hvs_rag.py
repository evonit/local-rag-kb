# SPDX-License-Identifier: Apache-2.0

"""
Description: Main application file for Hybrid Vector Search RAG pipeline.
"""


# hvs_rag.py
import json
import re
import time
from dataclasses import dataclass
from functools import partial
from typing import Any, Dict, List, Optional, Type, Tuple

from langchain_core.language_models import BaseChatModel
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel
from qdrant_client.http.models import ScoredPoint

# ëª¨ë¸/ì •ì±…/í”„ë¡¬í”„íŠ¸
from models.llm_policy import LLMPolicy
from pipelines.hvs_config import AgenticConfig
from prompts.prompt_store import PromptStore

# ë¦¬íŠ¸ë¦¬ë²Œ ìŠ¤íƒ(ì‚¬ìš©ìž í™˜ê²½ì— ë§žì¶˜ ëª¨ë“ˆ; ì§ì ‘ Qdrant + ë¦¬ëž­ì»¤)
from search.retrieval_text import HybridRetriever, QdrantSearcher, CrossEncoderReranker  # noqa: F401
from shared_types.llm_schema import ClarifyOut, QState, QInput


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AgenticRAGEngine
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _scored_point_list_to_json(r: list[ScoredPoint]) -> list[Dict[str, Any]]:
    return [{'id': p.id, 'score': p.score, 'payload': p.payload} for p in r]


@dataclass
class HybridVectorSearchRAG:
    policy: LLMPolicy
    prompt_store: PromptStore
    cfg: AgenticConfig
    emitter_factory: Any  # callable(stream_id) -> emitter(event/json/token)
    retriever: HybridRetriever


    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ê³µí†µ ìœ í‹¸(ìŠ¤íŠ¸ë¦¬ë° + JSON ì¶”ì¶œ)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    @staticmethod
    def _chunk_text(chunk) -> str:
        #print(type(chunk), chunk)
        return getattr(chunk, "content", "") # or getattr(chunk, "text", "")

    async def _stream_chain(self, prompt: ChatPromptTemplate, llm: BaseChatModel, inputs: dict, *,
                            emitter, node: str, stream_id: str) -> str:
        print(prompt)
        chain = prompt | llm
        parts: List[str] = []
        async for chunk in chain.astream(inputs):
            if not chunk:
                continue
            token = self._chunk_text(chunk)
            if token:
                parts.append(token)
                await emitter.token(node, stream_id, token)
        return "".join(parts)

    @staticmethod
    def _extract_json_block(text: str) -> Optional[Dict[str, Any]]:
        import re
        t = text.strip()
        m = re.search(r"```json(.*?)```", t, re.S | re.I)
        if m:
            try:
                return json.loads(m.group(1).strip())
            except Exception:
                pass
        for m in re.finditer(r"\{.*\}", t, re.S):
            try:
                return json.loads(m.group(0))
            except Exception:
                continue
        return None

    @staticmethod
    # JSONìš© citation ì •ë³´ ìƒì„±
    def make_citation_json(doc: Dict[str, Any], index: int) -> dict:
        metadata = doc.get('metadata', {})
        return {
            "index": index,
            "summary": doc.get('page_content', '').strip(),
            "source_file_name": metadata.get("source_file_name", ""),
            "source_subject": metadata.get("source_subject", ""),
            "source_id": metadata.get("source_id", ""),
            "page": metadata.get("page", 0),
        }

    @staticmethod
    async def _read_as_json(prompt: ChatPromptTemplate, llm: BaseChatModel, inputs: dict, schema: Type[BaseModel]) -> BaseModel:
        """schemaëŠ” Pydantic ëª¨ë¸ë¡œ, JSON ì¶œë ¥ í˜•ì‹ì„ ì •ì˜í•©ë‹ˆë‹¤."""
        print(prompt)
        structured_llm = llm.with_structured_output(schema)
        chain = prompt | structured_llm
        return await chain.ainvoke(inputs)

    async def _demo_mode(self, state: QState, config) -> bool:
        if state["user_query"] == "2015ë…„7ì›”1ì¼ ë¶ˆëŸ‰ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ í˜•íƒœë¡œ ë³´ë‚´ì¤˜":
            await self._demo_answer(state, config, self._demo_answer_1)
            return True
        elif state["user_query"] == "í•´ë‹¹ íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ìž¬ í•™ìŠµìš”ì²­í•˜ê³ , ë„ìž¥ë¶ˆëŸ‰ detect ì •í™•ë„ë¥¼ 3% ë‚®ì¶°ì¤˜":
            await self._demo_answer(state, config, self._demo_answer_2)
            return True
        return False

    async def _demo_answer(self, state: QState, config, answer):

        stream_id = state["stream_id"]
        node = "synthesize"
        emitter = self.emitter_factory(stream_id)
        await emitter.event(node, stream_id, "started")
        await self._stream_text(answer, emitter=emitter, node=node, stream_id=stream_id)
        await emitter.event(node, stream_id, "finished")
        state["ambiguous"] = True

    async def _stream_text(self, inputs: str, *, emitter, node: str, stream_id: str):
        # ë‹¨ì–´/êµ¬ë¶„ìž êµ¬ë¶„ì„ ìœ„í•´ kindë¥¼ í•¨ê»˜ ë°˜í™˜
        Token = Tuple[str, str, int, int]  # (kind, text, start, end)

        RE = re.compile(r"\S+|\s+")

        def tokenize(text: str) -> list[Token]:
            out = []
            for m in RE.finditer(text):
                frag = m.group(0)
                kind = "sep" if frag.isspace() else "word"
                out.append((kind, frag, m.start(), m.end()))
            return out

        tokens = tokenize(inputs)
        for _, tok, _, _ in tokens:
            await emitter.token(node, stream_id, tok)

    _demo_answer_1 = """
ìš”ì²­í•˜ì‹  2015ë…„ 7ì›” 1ì¼ ê¸°ì¤€ ë¶ˆëŸ‰ê²€ì¦ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œì´ ê²€ì¦ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤.
í•´ë‹¹ ë¦¬í¬íŠ¸ëŠ” PDF ë¬¸ì„œë¡œ ìž‘ì„±ë˜ì—ˆìœ¼ë©°, ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.

1. ê²€ì¦ ê°œìš” : ì´ ìƒì‚°ëŸ‰, ê²€ì¦ëœ ìƒ˜í”Œ ìˆ˜, ë¶ˆëŸ‰ ê²€ì¶œ ëŒ€ìƒ


2. ë¶ˆëŸ‰ ìœ í˜•ë³„ ê²€ì¶œ í˜„í™© : ë„ìž¥ë¶ˆëŸ‰, ìš©ì ‘ë¶ˆëŸ‰, ì¡°ë¦½ë¶ˆëŸ‰ ë“± ì£¼ìš” í•­ëª©ë³„ ê²°ê³¼


3. ê²€ì¶œ ì •í™•ë„ ë° ê°œì„  í•„ìš” ì˜ì—­ : ëª¨ë¸ ì •í™•ë„, ëª©í‘œ ëŒ€ë¹„ ì˜¤ì°¨ìœ¨, í›„ì† ê°œì„  ê¶Œê³  ì‚¬í•­


4. ì„¸ë¶€ ë¶„ì„ : ë¶ˆëŸ‰ ë°œìƒ ë¹ˆë„, ë¼ì¸ë³„ ë¶„í¬, ì‹œê°„ëŒ€ë³„ íŠ¹ì§•



ðŸ“‚ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ
ðŸ‘‰ http://3.34.126.162:3000/file/20150701/report_check.pdf


---

ðŸ“Š 2015ë…„ 7ì›” 1ì¼ ë¶ˆëŸ‰ê²€ì¦ ê²°ê³¼ ìš”ì•½ë³¸ (ì£¼ìš” ì§€í‘œ)

êµ¬ë¶„	ê²€ì¦ ê±´ìˆ˜	ë¶ˆëŸ‰ ê²€ì¶œ ê±´ìˆ˜	ê²€ì¶œìœ¨ (%)	ì£¼ìš” íŠ¹ì´ì‚¬í•­

ë„ìž¥ë¶ˆëŸ‰	1,200	96	8.0%	ì¼ë¶€ ë¼ì¸ì—ì„œ ë„ìž¥ ë‘ê»˜ ë¶ˆê· ì¼ í˜„ìƒ
ìš©ì ‘ë¶ˆëŸ‰	1,200	42	3.5%	íŠ¹ì • ìš©ì ‘ í¬ì¸íŠ¸ì—ì„œ ì§‘ì¤‘ ë°œìƒ
ì¡°ë¦½ë¶ˆëŸ‰	1,200	27	2.2%	ë¶€í’ˆ ì‚½ìž… ë¶ˆëŸ‰ì´ ë‹¤ìˆ˜ ë°œìƒ
ì´ê³„	1,200	165	13.7%	ì „ì²´ ê²€ì¶œìœ¨ì€ ì „ì›” ëŒ€ë¹„ 1.2% ìƒìŠ¹

ðŸ‘‰ ë³¸ ìš”ì•½ë³¸ì€ ì „ì²´ ë¦¬í¬íŠ¸ì˜ í•µì‹¬ ìˆ˜ì¹˜ë§Œ ì •ë¦¬í•œ ê°„ëžµí•œ ìžë£Œì´ë©°, ìƒì„¸ ë¶„ì„ ë° ê°œì„  ê¶Œê³  ì‚¬í•­ì€ PDF ë¦¬í¬íŠ¸ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

    _demo_answer_2 = """
ìš”ì²­í•˜ì‹  íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ìž¬í•™ìŠµ ìž‘ì—…ì´ ì˜ˆì•½ë˜ì—ˆìŠµë‹ˆë‹¤.
ë³¸ í•™ìŠµì€ 2015ë…„ 9ì›” 1ì¼ 00ì‹œì— ìžë™ìœ¼ë¡œ ì‹¤í–‰ë  ì˜ˆì •ìž…ë‹ˆë‹¤.

ë˜í•œ, ì‚¬ìš©ìž ìš”ì²­ì— ë”°ë¼ ë„ìž¥ë¶ˆëŸ‰(Defect: Painting) ê²€ì¶œ ì •í™•ë„ ê¸°ì¤€ì¹˜ë¥¼ ê¸°ì¡´ ëŒ€ë¹„ 3% ë‚®ì¶”ì–´ ì ìš©í•˜ë„ë¡ ì„¤ì •í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ì¡°ì •ìœ¼ë¡œ ì¸í•´, ê²½ê³„ê°’(Threshold)ì´ ì™„í™”ë˜ì–´ ë¶ˆëŸ‰ íƒì§€ ì‹œ ë¯¼ê°ë„ê°€ ë‚®ì•„ì§€ë©°, ê³¼ê²€ì¶œ(Over-detection) í˜„ìƒì„ ì¤„ì´ëŠ” íš¨ê³¼ê°€ ê¸°ëŒ€ë©ë‹ˆë‹¤.

ðŸ‘‰ í™•ì¸ì°¨ ë‹¤ì‹œ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤.

ìž¬í•™ìŠµ ê¸°ì¤€ íŒŒì¼ : ìš”ì²­ íŒŒì¼

í•™ìŠµ ì‹¤í–‰ ì¼ì • : 2015ë…„ 09ì›” 01ì¼ 00ì‹œ

ë„ìž¥ë¶ˆëŸ‰ Detect ì •í™•ë„ ì¡°ì • : ê¸°ì¡´ ëŒ€ë¹„ â€“3%
"""

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë…¸ë“œ ë©”ì„œë“œ (state, *, config) â€” ê³µìš© ì˜ì¡´ì„±ì€ selfì—ì„œ ì‚¬ìš©
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    async def clarify(self, state: QState, *, config) -> QState:

        if await self._demo_mode(state, config):
            return state

        stream_id = state["stream_id"]
        node = config["metadata"]["langgraph_node"]
        emitter = self.emitter_factory(stream_id)
        await emitter.event(node, stream_id, "started")

        print("[AgenticRAG] Clarifying user query: ", state["user_query"])

        prompt = self.prompt_store.chat_template("clarify")
        llm = self.policy.select(role="clarify", context={"input_tokens": len(state["user_query"]) // 4})

        clarify_out = await self._read_as_json(
            prompt, llm, {"question": state["user_query"]}, ClarifyOut) or ClarifyOut(ambiguous=False, ask="")
        print("[AgenticRAG] Clarify output: ", clarify_out)
        if clarify_out.ambiguous and clarify_out.ask:
            # ëª¨í˜¸í•œ ê²½ìš°, ì‚¬ìš©ìžì—ê²Œ ë‹¤ì‹œ ë¬¼ì–´ë³¼ ì§ˆë¬¸ì„ ë˜ì§€ê³  ì¢…ë£Œ
            await emitter.json(node, stream_id, {"ambiguous": True, "ask": clarify_out.ask})
            await emitter.event(node, stream_id, "finished")
        state["rewritten_query"] = clarify_out.rewritten_query
        state["sub_queries"] = clarify_out.sub_queries or []

        await emitter.json(node, stream_id, {"rewritten_query": state["rewritten_query"], "sub_queries": state["sub_queries"]})
        await emitter.event(node, stream_id, "finished")
        return state


    async def retrieve(self, state: QState, *, config) -> QState:
        stream_id = state["stream_id"]
        node = config["metadata"]["langgraph_node"]
        emitter = self.emitter_factory(stream_id)
        await emitter.event(node, stream_id, "started")

        req = [state["user_query"], state["rewritten_query"]] + state["sub_queries"]
        print(req)
        #batches = await asyncio.gather(*[self.retriever.aretrieve(q) for q in req], return_exceptions=False)
        batches = await self.retriever.aretrieve_many(req) #, filter_=state.get("filter"))
        hits_by_query = {q: _scored_point_list_to_json(r) for q, r in zip(req, batches)}
        state["hits_by_query"] = hits_by_query
        print(hits_by_query)
        await emitter.json(node, stream_id, {"hits_by_query": hits_by_query})
        await emitter.event(node, stream_id, "finished")
        return state


    async def synthesize(self, state: QState, *, config) -> QState:
        stream_id = state["stream_id"]
        node = config["metadata"]["langgraph_node"]
        emitter = self.emitter_factory(stream_id)
        await emitter.event(node, stream_id, "started")

        notes = []
        citation_json = []
        idx = 1
        for q, hits in state.get("hits_by_query", {}).items():
            for hit in hits:
                notes.append({"doc_idx": idx, "text": hit['payload']['page_content']})
                citation_json.append(self.make_citation_json(hit['payload'], idx))
                idx += 1

        prompt = self.prompt_store.chat_template("synthesize")
        llm = self.policy.select(role="synthesize")

        final_text = await self._stream_chain(
            prompt, llm, {
                "question": state["user_query"],
                "notes": json.dumps(notes, ensure_ascii=False),
            },
            emitter=emitter, node=node, stream_id=stream_id
        )
        state["final"] = final_text

        await emitter.json(node, state["stream_id"], {"final_answer": state.get("final", ""), "references": citation_json})
        await emitter.event(node, stream_id, "finished", {"chars": len(final_text)})
        return state

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ê·¸ëž˜í”„ ì»´íŒŒì¼
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def build_graph(self):
        g = StateGraph(QState, name="hybrid_vector_search_rag_graph")

        # partial(self.method)ë¡œ ë“±ë¡í•˜ë©´ LangGraphê°€ (state, config) í˜•íƒœë¡œ í˜¸ì¶œí•˜ë©°
        # configëŠ” ìžë™ ì£¼ìž…ë©ë‹ˆë‹¤.
        g.add_node("clarify",     partial(self.clarify))
        g.add_node("retrieve",    partial(self.retrieve))
        g.add_node("synthesize",  partial(self.synthesize))

        g.set_entry_point("clarify")

        g.add_conditional_edges("clarify", lambda state: END if state.get("ambiguous", False) else "retrieve")
        #g.add_edge("clarify", "retrieve")
        g.add_edge("retrieve", "synthesize")
        g.add_edge("synthesize", END)

        self.graph = g.compile()
        return self.graph


    # ====== ë‹¨ì¼ ì§ˆì˜ ì‹¤í–‰ í—¬í¼ ======
    async def run(self, query: str, stream_id: str, options: Optional[Dict] = None) -> QState:

        t0 = time.perf_counter()
        initial_state = QInput(user_query=query,stream_id=stream_id)

        if options:
            initial_state["options"] = options

        state = await self.graph.ainvoke(initial_state)

        dt = time.perf_counter() - t0
        print(f"\n\n[{stream_id}] ==== FINAL ({dt:.2f}s) ====\n{state.get('final','')}\n")
        return state

